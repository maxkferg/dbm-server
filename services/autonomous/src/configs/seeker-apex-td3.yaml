seeker-apex-td3:
    env: MultiRobot-v0
    run: APEX_DDPG
    checkpoint_freq: 50
    stop:
        episode_reward_mean: 5
    config:
        # === Tricks ====
        twin_q: True
        policy_delay: 2
        smooth_target_policy: True
        target_noise: 0.2
        horizon: 100

        # === Model ===
        eager: False
        actor_hiddens: [400, 300]
        critic_hiddens: [400, 300]
        n_step: 3
        gamma: 0.99
        env_config: {}
        model: {
            custom_model: "fusion_model",
            custom_options: {}
        }

        # === Exploration ===
        schedule_max_timesteps: 20000000 # 100 million
        exploration_fraction: 0.5
        target_network_update_freq: 20000
        tau: 0.001 # 1

        # Number of CPUs to allocate for the trainer. Note: this only takes effect
        # when running in Tune.
        num_cpus_for_driver": 10

        # You can set these memory quotas to tell Ray to reserve memory for your
        # training run. This guarantees predictable execution, but the tradeoff is
        # if your workload exceeeds the memory quota it will fail.
        # Heap memory to reserve for the trainer process (0 for unlimited). This
        # can be large if your are using large train batches, replay buffers, etc.
        memory: 5000000000 #50Gb
        object_store_memory: 3000000000 #30GB
        memory_per_worker: 2000000000 # 2GB

        # === Replay buffer ===
        buffer_size: 100000
        prioritized_replay: True
        prioritized_replay_alpha: 0.6
        prioritized_replay_beta: 0.4
        prioritized_replay_eps: 0.000001
        clip_rewards: False
        optimizer:
            num_replay_buffer_shards: 8

        # === Optimization ===
        lr: 0.0002
        use_huber: True
        huber_threshold: 1.0
        l2_reg: 0.0000001
        learning_starts: 100000
        sample_batch_size: 64
        train_batch_size: 1024
        min_iter_time_s: 30

        # === Parallelism ===
        #num_gpus: 1
        num_gpus: 0
        num_workers: 32
        num_cpus_per_worker: 1
        batch_mode: "truncate_episodes"
        num_envs_per_worker: 8
        remote_worker_envs: True
        ignore_worker_failures: True
