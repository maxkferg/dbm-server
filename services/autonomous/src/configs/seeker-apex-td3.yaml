seeker-apex-td3:
    run: APEX_DDPG
    checkpoint_freq: 50
    stop:
        episode_reward_mean: 5
    config:
        # === Environment ===
        env: MultiRobot-v0
        env_config:
            headless: True

        # === Tricks ====
        twin_q: True
        policy_delay: 2
        smooth_target_policy: True
        target_noise: 0.2
        target_noise_clip: 0.5
        horizon: 100

        # === Model ===
        eager: False
        n_step: 3
        gamma: 0.99
        use_state_preprocessor: True
        model:
            use_lstm: true
            custom_model: "fusion_model"
            custom_options: {}

        # === Eval ===
        #evaluation_interval: 5
        #evaluation_num_episodes: 10

        # === Exploration ===
        schedule_max_timesteps: 20000000 # 100 million
        exploration_noise_type: "gaussian"
        exploration_should_anneal: False
        exploration_gaussian_sigma: 0.1

        # === Replay buffer ===
        buffer_size: 100000
        prioritized_replay: True
        prioritized_replay_alpha: 0.6
        prioritized_replay_beta: 0.4
        prioritized_replay_eps: 0.000001
        clip_rewards: False
        optimizer:
            num_replay_buffer_shards: 8

        # === Optimization ===
        l2_reg: 0.0
        actor_lr: 1e-3,
        critic_lr: 1e-3,
        learning_starts: 50000
        pure_exploration_steps: 50000
        per_worker_exploration: False
        sample_batch_size: 20
        train_batch_size: 512
        min_iter_time_s: 30

        # === Target Network ===
        target_network_update_freq: 0
        tau: 0.005 # 1

        # === Parallelism ===
        num_gpus: 1
        num_workers: 8
        num_cpus_per_worker: 4
        batch_mode: "truncate_episodes"
        num_envs_per_worker: 8
        remote_worker_envs: True